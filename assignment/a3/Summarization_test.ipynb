{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-k1jXQHLyFU"
      },
      "source": [
        "# Assignment 3: Summarization Tests\n",
        "\n",
        "**Description:** This assignment covers summarization outputs. You will compare three different types of solutions, all using an encoder decoder architecture. You should also be able to develop an intuition for:\n",
        "\n",
        "\n",
        "* How well summarization systems work\n",
        "* The effects of using different pre-training and fine-tuning checkpoints on outcomes\n",
        "* The effects of hyperparameters on outcomes\n",
        "* Evaluation of output using ROUGE\n",
        "\n",
        "\n",
        "\n",
        "This notebook should be run on a Google Colab but it does not require a GPU. By default, when you open the notebook in Colab it will NOT configure a GPU.  Summarization commands can take up to five minutes to run depending on the hyperparameters you use. This notebook will NOT run on your GCP instance as the summary models are larger than the avaialble memory.\n",
        "\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/datasci-w266/2023-summer-main/blob/master/assignment/a3/Summarization_test.ipynb)\n",
        "\n",
        "The overall assignment structure is as follows:\n",
        "\n",
        " Setup\n",
        "\n",
        "1. T5 for generic summarization\n",
        "\n",
        "2. Pegasus for headline summarization\n",
        "\n",
        "3. Pegasus for longer generation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**INSTRUCTIONS:**:\n",
        "\n",
        "* Questions are always indicated as **QUESTION:**, so you can search for this string to make sure you answered all of the questions. You are expected to fill out, run, and submit this notebook, as well as to answer the questions in the **answers** file as you did in a1 and a2.\n",
        "\n",
        "* **### YOUR CODE HERE** indicates that you are supposed to write code.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QDMDB_KqT8m"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Gl5EZTtfK5jL"
      },
      "outputs": [],
      "source": [
        "!pip install -q sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xp3SaiO8LXrC"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KzQBFiaPnr4j"
      },
      "outputs": [],
      "source": [
        "!pip install -q evaluate\n",
        "import evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Lsz88WynuVkn"
      },
      "outputs": [],
      "source": [
        "!pip install -q rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "M9Z1Xt8YIS-6"
      },
      "outputs": [],
      "source": [
        "#let's make longer output readable without horizontal scrolling\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CE4JuNugLura"
      },
      "source": [
        "Let's leverage the pre-trained and fine tuned models on HuggingFace to demonstrate some capabilities with abstractive summarization and language generation.  They include models/checkpoints that were fine tuned on a particular dataset.  In our case we'll focus on one dataset that emphasizes a one line output and another that emphasizes a multi-line output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UUcHy8gPpHT"
      },
      "source": [
        "We'll use this same toy article as the input to all of our summarization attempts.  That way we have the ability to compare. We'll also create two references for evaluation.  These are the targets you are trying to meet.  One reference is for the longer output. The second reference is the short one for the one line output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BO8pkrxNLYIz"
      },
      "outputs": [],
      "source": [
        "\n",
        "ARTICLE_TO_SUMMARIZE = (\n",
        "    \"Nearly 800 thousand customers are scheduled to be affected by the shutoffs which are expected to last through at least midday tomorrow. \"\n",
        "    \"PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions. \"\n",
        "    \"The aim is to reduce the risk of wildfires. \"\n",
        "    \"If Pacific Gas & Electric Co, a unit of PG&E Corp, goes through with another public safety power shutoff, \"\n",
        "    \" it would be the fourth round of mass blackouts imposed by the utility since Oct. 9, when some 730,000 customers were left in the dark. \"\n",
        "    \"The recent wave of precautionary shutoffs have drawn sharp criticism from Governor Gavin Newsom, state regulators and consumer activists as being overly broad in scale.\"\n",
        "    \"Newsom blames PG&E for doing too little to properly maintain and secure its power lines against wind damage.\"\n",
        "    \"Utility executives have acknowledged room for improvement while defending the sprawling scope of the power cutoffs as a matter of public safety.\"\n",
        "    \"The record breaking drought has made the current conditions even worse than in previous years. \"\n",
        "    \"It exponentially increases the probability of large scale wildfires. \"\n",
        ")\n",
        "\n",
        "LONG_REFERENCE = (\n",
        "    \"Many PG&E customers could be affected by public safety power shutoffs in response to forecasts for high winds and dry conditions. \"\n",
        "    \"The record breaking drought exponentially increases the probability of large scale wildfires. \"\n",
        "    \"Despite being criticized by Governor Newsom for being overly broad, company officials defend the cutoffs as a matter of public safety. \"\n",
        ")\n",
        "\n",
        "SHORT_REFERENCE = (\n",
        "    \"California's largest utility is set to turn off power to hundreds of thousands of customers in an effort to reduce the risk of wildfires. \"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gImnMrnCzHa"
      },
      "source": [
        "How long is our article to summarize?  Obviously our summary should be shorter since it is supposed to be \"abridged.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yg7Hf_VdaQj-",
        "outputId": "1d6937f6-81da-48e2-d5c4-5c958add079b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "177"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "len(str.split(ARTICLE_TO_SUMMARIZE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9xudUadMrFA"
      },
      "source": [
        "## 1. T5 for Generic Summarization\n",
        "\n",
        "T5 is an encoder decoder architecture that has been trained on multiple tasks, so not purely summarization.  You can read more about it [here](https://huggingface.co/docs/transformers/model_doc/t5)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acxPLRJiSzXq",
        "outputId": "77d0e0bc-446b-4336-c11c-40bc54561c82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        }
      ],
      "source": [
        "from transformers import T5Tokenizer, TFT5ForConditionalGeneration\n",
        "\n",
        "t5model = TFT5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
        "t5tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bA8lh3lEO0C",
        "outputId": "76fff844-61e4-4b80-c70e-d37305ea7c60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tft5_for_conditional_generation\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " shared (Embedding)          multiple                  24674304  \n",
            "                                                                 \n",
            " encoder (TFT5MainLayer)     multiple                  109628544 \n",
            "                                                                 \n",
            " decoder (TFT5MainLayer)     multiple                  137949312 \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 222903552 (850.31 MB)\n",
            "Trainable params: 222903552 (850.31 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "t5model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYrpx6lSQu6K"
      },
      "source": [
        "Since T5 can perform multiple tasks we need to tell it what kind of output we want.  Therefore we need to prepend a \"prompt\" to our article text to make sure it does the right thing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "haKG84-iNe9k"
      },
      "outputs": [],
      "source": [
        "PROMPT = 'summarize: '\n",
        "T5ARTICLE_TO_SUMMARIZE = PROMPT + ARTICLE_TO_SUMMARIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "EKgvAz84SzSH"
      },
      "outputs": [],
      "source": [
        "inputs = t5tokenizer(T5ARTICLE_TO_SUMMARIZE, max_length=1024, truncation=True, return_tensors=\"tf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoDo-7DzQCQl"
      },
      "source": [
        "What do the inputs look like?  How does it compare with what we've seen from BERT?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "il9ui27nSRID",
        "outputId": "402b227e-c7d9-4a6d-fe25-5670125dfcf2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': <tf.Tensor: shape=(1, 241), dtype=int32, numpy=\n",
              "array([[21603,    10, 10455,   120,  8640,  7863,   722,    33,  5018,\n",
              "           12,    36,  4161,    57,     8,  6979,  1647,     7,    84,\n",
              "           33,  1644,    12,   336,   190,    44,   709,  2076,  1135,\n",
              "         5721,     5,     3,  7861,   184,   427,  4568,    34,  5018,\n",
              "            8,  1001,   670,     7,    16,  1773,    12,  7555,     7,\n",
              "           21,   306, 13551, 18905,  2192,  1124,     5,    37,  2674,\n",
              "           19,    12,  1428,     8,  1020,    13,  3645,  6608,     7,\n",
              "            5,   156,  5824,  6435,     3,   184,  8666,   638,     6,\n",
              "            3,     9,  1745,    13,     3,  7861,   184,   427, 10052,\n",
              "            6,  1550,   190,    28,   430,   452,  1455,   579,  6979,\n",
              "         1647,     6,    34,   133,    36,     8,  4509,  1751,    13,\n",
              "         3294,  1001,   670,     7,     3, 16068,    57,     8,  6637,\n",
              "          437,  6416,     5,  9902,   116,   128,   489, 17093,   722,\n",
              "          130,   646,    16,     8,  2164,     5,    37,  1100,  6772,\n",
              "           13, 21059,  1208,  6979,  1647,     7,    43,  6796,  4816,\n",
              "        12334,    45, 10510,  2776,  2494,   368, 10348,     6,   538,\n",
              "        14415,     7,    11,  3733, 19053,    38,   271,   147,   120,\n",
              "         4358,    16,  2643,     5,  6861, 10348,  9100,     7,     3,\n",
              "         7861,   184,   427,    21,   692,   396,   385,    12,  3085,\n",
              "         1961,    11,  2451,   165,   579,  2356,   581,  2943,  1783,\n",
              "            5,  1265,    17, 14277, 13510,    43, 16974,   562,    21,\n",
              "         4179,   298,     3, 20309,     8, 28880,    53,  7401,    13,\n",
              "            8,   579,  1340,  1647,     7,    38,     3,     9,  1052,\n",
              "           13,   452,  1455,     5,   634,  1368,  7814, 19611,    65,\n",
              "          263,     8,   750,  1124,   237,  4131,   145,    16,  1767,\n",
              "          203,     5,    94, 25722,   120,  5386,     8, 15834,    13,\n",
              "          508,  2643,  3645,  6608,     7,     5,     1]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 241), dtype=int32, numpy=\n",
              "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
              "      dtype=int32)>}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6NT_ADVQCQm"
      },
      "source": [
        "Let's just run T5 using it's default hyperparameters and see what happens.  We'll hold on to the output in the candidate variable.  What do you think about the output?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQzPxfCutzsl",
        "outputId": "fc539e40-3c77-4630-aee6-603b5a58a162"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('PG&e shuts off power to 800 thousand customers . the shutoffs are scheduled '\n",
            " 'to last through at least midday tomorrow .')\n"
          ]
        }
      ],
      "source": [
        "# Generate Summary\n",
        "summary_ids = t5model.generate(inputs[\"input_ids\"],\n",
        "                               max_new_tokens=30\n",
        ")\n",
        "candidate = t5tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
        "pprint(candidate[0], compact=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JSmlLkqtHQ5"
      },
      "source": [
        "### 1.a Checkpoint Configuration\n",
        "\n",
        "We're using the `t5-base` configuration and we know we can run out of the box to do summarization which means it has some hyperparameters set as defaults.  These may or may not be what we want to use.  How do we know which values are set as defaults?\n",
        "\n",
        "HuggingFace provides access to the default hyperparameters via the AutoConfig object which we call below.  We simply pass in the name of the checkpoint we're using -- `t5-base` in this case.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-DC5UoBscv4",
        "outputId": "a640e525-7f73-42ec-ceec-80db26549cc3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "T5Config {\n",
              "  \"architectures\": [\n",
              "    \"T5ForConditionalGeneration\"\n",
              "  ],\n",
              "  \"classifier_dropout\": 0.0,\n",
              "  \"d_ff\": 3072,\n",
              "  \"d_kv\": 64,\n",
              "  \"d_model\": 768,\n",
              "  \"decoder_start_token_id\": 0,\n",
              "  \"dense_act_fn\": \"relu\",\n",
              "  \"dropout_rate\": 0.1,\n",
              "  \"eos_token_id\": 1,\n",
              "  \"feed_forward_proj\": \"relu\",\n",
              "  \"initializer_factor\": 1.0,\n",
              "  \"is_encoder_decoder\": true,\n",
              "  \"is_gated_act\": false,\n",
              "  \"layer_norm_epsilon\": 1e-06,\n",
              "  \"model_type\": \"t5\",\n",
              "  \"n_positions\": 512,\n",
              "  \"num_decoder_layers\": 12,\n",
              "  \"num_heads\": 12,\n",
              "  \"num_layers\": 12,\n",
              "  \"output_past\": true,\n",
              "  \"pad_token_id\": 0,\n",
              "  \"relative_attention_max_distance\": 128,\n",
              "  \"relative_attention_num_buckets\": 32,\n",
              "  \"task_specific_params\": {\n",
              "    \"summarization\": {\n",
              "      \"early_stopping\": true,\n",
              "      \"length_penalty\": 2.0,\n",
              "      \"max_length\": 200,\n",
              "      \"min_length\": 30,\n",
              "      \"no_repeat_ngram_size\": 3,\n",
              "      \"num_beams\": 4,\n",
              "      \"prefix\": \"summarize: \"\n",
              "    },\n",
              "    \"translation_en_to_de\": {\n",
              "      \"early_stopping\": true,\n",
              "      \"max_length\": 300,\n",
              "      \"num_beams\": 4,\n",
              "      \"prefix\": \"translate English to German: \"\n",
              "    },\n",
              "    \"translation_en_to_fr\": {\n",
              "      \"early_stopping\": true,\n",
              "      \"max_length\": 300,\n",
              "      \"num_beams\": 4,\n",
              "      \"prefix\": \"translate English to French: \"\n",
              "    },\n",
              "    \"translation_en_to_ro\": {\n",
              "      \"early_stopping\": true,\n",
              "      \"max_length\": 300,\n",
              "      \"num_beams\": 4,\n",
              "      \"prefix\": \"translate English to Romanian: \"\n",
              "    }\n",
              "  },\n",
              "  \"transformers_version\": \"4.52.4\",\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 32128\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "from transformers import AutoConfig\n",
        "\n",
        "config = AutoConfig.from_pretrained(\"t5-base\")\n",
        "\n",
        "config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqhE0DpTvmQP"
      },
      "source": [
        "Look at the `task_specific_params` for summarization. You can see that this `t5-base` checkpoint has some values such as min_length and max_length as well as no_repeat_ngram_size and num_beams.  You can affect the size and content of the output by modifying these parameters which you will do below.\n",
        "\n",
        "You can also look at the full set of possible parameters in the [TFGenerationMixin](https://huggingface.co/docs/transformers/v4.18.0/en/main_classes/text_generation#transformers.generation_tf_utils.TFGenerationMixin) class available to all of the pre-trained models.\n",
        "\n",
        "HuggingFace has also written [a very helpful blog post](https://huggingface.co/blog/how-to-generate) that explains and discusses various strategies for text generation and how to manipulate the hyperparameters.  They discuss the two approaches of beam search (which we have discussed in the async and live session) as well as sampling (which tries to randomly pick the next word within a k-sized distribution of highly probable choices).\n",
        "\n",
        "**Please read the blog post before you proceed.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMFFnkUwRWKi"
      },
      "source": [
        "For your reference, here's a more complex, technical, and thorough [HuggingFace guide](https://huggingface.co/docs/transformers/main/en/generation_strategies) for controlling generation of text.  The blog post above is all you need to read to complete the assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C89LpeBj7i4F"
      },
      "source": [
        "### 1.b ROUGE for summarization evaluation\n",
        "\n",
        "ROUGE is the metric that has been traditionally used to evaluate sumarization results.  The ROUGE metric expects a reference as input and it will evaluate a candidate against that reference.  ROUGE-1 calculates the number of words in the reference that occur in the candidate.  ROUGE-2 performs that same calculation but for bigrams in the reference. ROUGE-L calculates the longest common subsequence of reference words that occur in the candidate.\n",
        "\n",
        "HuggingFace provides a wrapper around [a library](https://huggingface.co/spaces/evaluate-metric/rouge) to calculate ROUGE metrics which you will use below.  Let's calculate the ROUGE score for the candidate you produced above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ae6-_06inr44",
        "outputId": "c899a494-9434-436e-e591-f0a36cb05cfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rouge1': np.float64(0.2666666666666666), 'rouge2': np.float64(0.0930232558139535), 'rougeL': np.float64(0.22222222222222224), 'rougeLsum': np.float64(0.22222222222222224)}\n"
          ]
        }
      ],
      "source": [
        "rouge = evaluate.load('rouge')\n",
        "predictions = candidate\n",
        "references = [SHORT_REFERENCE]\n",
        "results = rouge.compute(predictions=predictions,\n",
        "                        references=references)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8OQmtGeQCQp"
      },
      "source": [
        "Let's experiment with the hyperparameters shown above.  Please experiment in the cell below.  The `num_beams` value is like a beam search.  It indicates the number of tries the model makes before showing you its best output.  The `no_repeat_ngram_size` is designed to help reduce repetition in the output.  `min_length` and `max_length` set boundaries on the size of the summary. You are free to use other hyperparameters as described in the [blog post](https://huggingface.co/blog/how-to-generate).\n",
        "\n",
        "*There is no one correct answer to these questions.  There are ranges that tend to work better than others.  The goal is to have you experiment to help build intuition.  Please enter the values that you think are generating the most readable output.*\n",
        "\n",
        "*Your readable output should consist of at least one complete sentence but does not have to end with a period and you must also have a ROUGE-1 score above 0.30 and ROUGE-L score equal to or above 0.25 when compared with the short reference.*\n",
        "\n",
        "You can use the two cells below to come up with your answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buSASaloC305",
        "outputId": "b2c7aaea-71e1-414d-c547-033b1068eaec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('PG&e shuts off power to 800 thousand customers in response to forecasts for '\n",
            " 'high winds . the aim is to reduce the risk')\n"
          ]
        }
      ],
      "source": [
        "# Generate Summary\n",
        "\n",
        "### YOUR CODE HERE\n",
        "summary_ids = t5model.generate(\n",
        "    inputs[\"input_ids\"],\n",
        "    num_beams=4,\n",
        "    no_repeat_ngram_size=2,\n",
        "    min_length=20,\n",
        "    max_length=30,\n",
        "    early_stopping=True\n",
        ")\n",
        "### END YOUR CODE\n",
        "\n",
        "candidate = t5tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
        "pprint(candidate[0], compact=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckbLXpnbQCQq",
        "outputId": "8cae11ff-b5d6-431c-fc92-69a5e6e44d61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rouge1': np.float64(0.4583333333333333), 'rouge2': np.float64(0.2608695652173913), 'rougeL': np.float64(0.37499999999999994), 'rougeLsum': np.float64(0.37499999999999994)}\n"
          ]
        }
      ],
      "source": [
        "predictions = candidate\n",
        "references = [SHORT_REFERENCE]\n",
        "results = rouge.compute(predictions=predictions,\n",
        "                        references=references)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ml2BkDi4UtvM"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "1.1 What num_beams value gives you the most readable output that meets the score criteria?\n",
        "\n",
        "1.2 Which no_repeat_ngram_size gives the most readable output that meets the score criteria?\n",
        "\n",
        "1.3 What min_length value gives you the most readable output that meets the score criteria?\n",
        "\n",
        "1.4 Which max_new_tokens value gives the most readable output that meets the score criteria?\n",
        "\n",
        "1.5 What is the ROUGE-L score associated with your most readable candidate?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "RIY8uph4iyjW"
      },
      "outputs": [],
      "source": [
        "#In order to not consume all of the memory available in Colab we'll free up the memory we're using for these large language models\n",
        "del t5model\n",
        "del t5tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiloUXq4SIOB"
      },
      "source": [
        "\n",
        "## 2. Pegasus for Headline Summarization\n",
        "\n",
        "Pegasus is an encoder decoder architecture that has been explicitly pre-trained as an abstractive summarizer.  You can read more about it [here](https://huggingface.co/docs/transformers/model_doc/pegasus) and [here](https://arxiv.org/pdf/1912.08777.pdf).\n",
        "\n",
        "We'll first use the `google/pegasus-xsum` checkpoint.  It is trained on a [summarization task](https://aclanthology.org/D18-1206.pdf) that reads a news article and then [emits a one line summary](https://huggingface.co/datasets/xsum).  This doesn't mean that it is limited in its output length.  It does mean that it works well with news article type inputs and tends toward shorter outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aL_vm5XBLYCa",
        "outputId": "636c3d75-0749-4d07-ada5-a54439173d2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFPegasusForConditionalGeneration.\n",
            "\n",
            "Some layers of TFPegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['final_logits_bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import PegasusTokenizer, TFPegasusForConditionalGeneration\n",
        "\n",
        "pmodel = TFPegasusForConditionalGeneration.from_pretrained(\"google/pegasus-xsum\")\n",
        "ptokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-xsum\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihtp3HlGSRZF",
        "outputId": "963cc225-1a85-4a88-feff-7373de66a281"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tf_pegasus_for_conditional_generation\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " model (TFPegasusMainLayer)  multiple                  569748480 \n",
            "                                                                 \n",
            " final_logits_bias (BiasLay  multiple                  96103     \n",
            " er)                                                             \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 569844583 (2.12 GB)\n",
            "Trainable params: 569748480 (2.12 GB)\n",
            "Non-trainable params: 96103 (375.40 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "pmodel.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ymuHNVDQCQt"
      },
      "source": [
        "Let's see what kinds of default parameters are configured in to this checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOP1XnpyLKNM",
        "outputId": "4c82ba9e-9ac6-4d21-f5ba-fd2e0dec401f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PegasusConfig {\n",
              "  \"activation_dropout\": 0.1,\n",
              "  \"activation_function\": \"relu\",\n",
              "  \"add_bias_logits\": false,\n",
              "  \"add_final_layer_norm\": true,\n",
              "  \"architectures\": [\n",
              "    \"PegasusForConditionalGeneration\"\n",
              "  ],\n",
              "  \"attention_dropout\": 0.1,\n",
              "  \"bos_token_id\": 0,\n",
              "  \"classif_dropout\": 0.0,\n",
              "  \"classifier_dropout\": 0.0,\n",
              "  \"d_model\": 1024,\n",
              "  \"decoder_attention_heads\": 16,\n",
              "  \"decoder_ffn_dim\": 4096,\n",
              "  \"decoder_layerdrop\": 0.0,\n",
              "  \"decoder_layers\": 16,\n",
              "  \"decoder_start_token_id\": 0,\n",
              "  \"do_blenderbot_90_layernorm\": false,\n",
              "  \"dropout\": 0.1,\n",
              "  \"encoder_attention_heads\": 16,\n",
              "  \"encoder_ffn_dim\": 4096,\n",
              "  \"encoder_layerdrop\": 0.0,\n",
              "  \"encoder_layers\": 16,\n",
              "  \"eos_token_id\": 1,\n",
              "  \"extra_pos_embeddings\": 0,\n",
              "  \"force_bos_token_to_be_generated\": false,\n",
              "  \"forced_eos_token_id\": 1,\n",
              "  \"gradient_checkpointing\": false,\n",
              "  \"id2label\": {\n",
              "    \"0\": \"LABEL_0\",\n",
              "    \"1\": \"LABEL_1\",\n",
              "    \"2\": \"LABEL_2\"\n",
              "  },\n",
              "  \"init_std\": 0.02,\n",
              "  \"is_encoder_decoder\": true,\n",
              "  \"label2id\": {\n",
              "    \"LABEL_0\": 0,\n",
              "    \"LABEL_1\": 1,\n",
              "    \"LABEL_2\": 2\n",
              "  },\n",
              "  \"length_penalty\": 0.6,\n",
              "  \"max_length\": 64,\n",
              "  \"max_position_embeddings\": 512,\n",
              "  \"model_type\": \"pegasus\",\n",
              "  \"normalize_before\": true,\n",
              "  \"normalize_embedding\": false,\n",
              "  \"num_beams\": 8,\n",
              "  \"num_hidden_layers\": 16,\n",
              "  \"pad_token_id\": 0,\n",
              "  \"scale_embedding\": true,\n",
              "  \"static_position_embeddings\": true,\n",
              "  \"transformers_version\": \"4.52.4\",\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 96103\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "config = AutoConfig.from_pretrained(\"google/pegasus-xsum\")\n",
        "\n",
        "config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IelLGg6jQCQt"
      },
      "source": [
        "Generate the inputs using the pegasus tokenizer for this checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "WiqSwOmD_ms_"
      },
      "outputs": [],
      "source": [
        "inputs = ptokenizer(ARTICLE_TO_SUMMARIZE, max_length=1024, truncation=True, return_tensors=\"tf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FU_DtcjjQCQu"
      },
      "source": [
        "Let's get some output using just the default values and see what we're working with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xibRiEDW5SoG",
        "outputId": "aa1851b0-f038-4f8f-8d37-cf47dc570512"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(\"California's largest utility has announced plans to cut power to hundreds of \"\n",
            " 'thousands of customers in a bid to reduce the risk of wildfires.')\n"
          ]
        }
      ],
      "source": [
        "# Generate Summary\n",
        "summary_ids = pmodel.generate(inputs[\"input_ids\"]\n",
        ")\n",
        "pprint(ptokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0], compact=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUExOejWXQRl"
      },
      "source": [
        "Let's experiment with the same set of hyperparameters for the Pegasus system.  It is designed for abstractive summarization. Remember that the checkpoint we are using was trained on data that generates a one line summary for the input article.\n",
        "\n",
        "*Your readable output should consist of at least one complete sentence but does not have to end with a period and you must also have a ROUGE-1 score above 0.30 and ROUGE-L score equal to or above 0.25 when compared with the short reference.*\n",
        "\n",
        "You can use the two cells below to experiment with hyperparameters and generating and scoring your outputs in order to answer questions 2.1 - 2.5 in your answers file.\n",
        "\n",
        "**QUESTION:**\n",
        "\n",
        "2.1 What num_beams value gives you the most readable output that meets the score criteria?\n",
        "\n",
        "2.2 Which no_repeat_ngram_size gives the most readable output that meets the score criteria?\n",
        "\n",
        "2.3 What min_length value gives you the most readable output that meets the score criteria?\n",
        "\n",
        "2.4 Which max_new_tokens value gives the most readable output that meets the score criteria?\n",
        "\n",
        "2.5 What is the ROUGE-L score associated with your most readable candidate?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "UzwtfRgFLYPD",
        "outputId": "86999993-e0e4-4de1-f100-5568a4554e7e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pmodel' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-35-4102708602.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m### YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m ummary_ids = pmodel.generate(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mnum_beams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m               \u001b[0;31m# 2.1: beam width of 4 gives a good balance of exploration and fluency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pmodel' is not defined"
          ]
        }
      ],
      "source": [
        "# Generate Summary\n",
        "\n",
        "### YOUR CODE HERE\n",
        "ummary_ids = pmodel.generate(\n",
        "    inputs[\"input_ids\"],\n",
        "    num_beams=4,               # 2.1: beam width of 4 gives a good balance of exploration and fluency\n",
        "    no_repeat_ngram_size=2,    # 2.2: blocking 2-gram repeats helps reduce redundancy\n",
        "    min_length=20,             # 2.3: at least 20 tokens to ensure a full sentence\n",
        "    max_new_tokens=30,         # 2.4: up to 30 new tokens produces a concise one-liner\n",
        "    early_stopping=True\n",
        ")\n",
        "### END YOUR CODE\n",
        ")\n",
        "candidate = ptokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
        "pprint(candidate[0], compact=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsZHkhZ0SzM8",
        "outputId": "e2985182-25ee-4fc8-84c3-dfae18ac3dfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rouge1': np.float64(0.76), 'rouge2': np.float64(0.625), 'rougeL': np.float64(0.76), 'rougeLsum': np.float64(0.76)}\n"
          ]
        }
      ],
      "source": [
        "rouge = evaluate.load('rouge')\n",
        "predictions = candidate\n",
        "references = [SHORT_REFERENCE]\n",
        "results = rouge.compute(predictions=predictions,\n",
        "                        references=references)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcCCWke-ulaz"
      },
      "source": [
        "Delete that Pegasus model and tokenizer so we can load the next one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "00Ka26vgk_FT"
      },
      "outputs": [],
      "source": [
        "del pmodel\n",
        "del ptokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eXBMGbxtkkh"
      },
      "source": [
        "## 3. Pegasus for Longer Generation\n",
        "\n",
        "Now let's try to produce a longer summary of our article.  In order to do that we are going to use a different fine-tuned checkpoint for Pegasus.  This checkpoint is fine-tuned on the [CNN/Daily Mail](https://huggingface.co/datasets/cnn_dailymail) set of news articles.  The references are on the order of several sentences long."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OME5_uYKbvwL",
        "outputId": "008c8992-eefc-46e9-9427-6935018e5a2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFPegasusForConditionalGeneration.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFPegasusForConditionalGeneration were not initialized from the PyTorch model and are newly initialized: ['model.encoder.embed_positions.weight', 'model.decoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import PegasusTokenizer, TFPegasusForConditionalGeneration\n",
        "\n",
        "cnnmodel = TFPegasusForConditionalGeneration.from_pretrained(\"google/pegasus-cnn_dailymail\", from_pt=True)\n",
        "cnntokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-cnn_dailymail\", from_pt=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNeNhSWSQCQx"
      },
      "source": [
        "Let's see how this checkpoint is configured by default:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EOe_hLKLaOR",
        "outputId": "ab8e5b5b-4dcb-4733-f2fc-00651f1ad1ad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PegasusConfig {\n",
              "  \"activation_dropout\": 0.1,\n",
              "  \"activation_function\": \"relu\",\n",
              "  \"add_bias_logits\": false,\n",
              "  \"add_final_layer_norm\": true,\n",
              "  \"architectures\": [\n",
              "    \"PegasusForConditionalGeneration\"\n",
              "  ],\n",
              "  \"attention_dropout\": 0.1,\n",
              "  \"bos_token_id\": 0,\n",
              "  \"classif_dropout\": 0.0,\n",
              "  \"d_model\": 1024,\n",
              "  \"decoder_attention_heads\": 16,\n",
              "  \"decoder_ffn_dim\": 4096,\n",
              "  \"decoder_layerdrop\": 0.0,\n",
              "  \"decoder_layers\": 16,\n",
              "  \"decoder_start_token_id\": 0,\n",
              "  \"dropout\": 0.1,\n",
              "  \"encoder_attention_heads\": 16,\n",
              "  \"encoder_ffn_dim\": 4096,\n",
              "  \"encoder_layerdrop\": 0.0,\n",
              "  \"encoder_layers\": 16,\n",
              "  \"eos_token_id\": 1,\n",
              "  \"extra_pos_embeddings\": 1,\n",
              "  \"forced_eos_token_id\": 1,\n",
              "  \"id2label\": {\n",
              "    \"0\": \"LABEL_0\",\n",
              "    \"1\": \"LABEL_1\",\n",
              "    \"2\": \"LABEL_2\"\n",
              "  },\n",
              "  \"init_std\": 0.02,\n",
              "  \"is_encoder_decoder\": true,\n",
              "  \"label2id\": {\n",
              "    \"LABEL_0\": 0,\n",
              "    \"LABEL_1\": 1,\n",
              "    \"LABEL_2\": 2\n",
              "  },\n",
              "  \"length_penalty\": 0.8,\n",
              "  \"max_length\": 128,\n",
              "  \"max_position_embeddings\": 1024,\n",
              "  \"min_length\": 32,\n",
              "  \"model_type\": \"pegasus\",\n",
              "  \"normalize_before\": true,\n",
              "  \"normalize_embedding\": false,\n",
              "  \"num_beams\": 8,\n",
              "  \"num_hidden_layers\": 16,\n",
              "  \"pad_token_id\": 0,\n",
              "  \"scale_embedding\": true,\n",
              "  \"static_position_embeddings\": true,\n",
              "  \"transformers_version\": \"4.52.4\",\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 96103\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "config = AutoConfig.from_pretrained(\"google/pegasus-cnn_dailymail\")\n",
        "\n",
        "config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcedEgr2QCQx"
      },
      "source": [
        "Let's tokenize our input for this checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "FWl1UypibvkS"
      },
      "outputs": [],
      "source": [
        "cnninputs = cnntokenizer(ARTICLE_TO_SUMMARIZE, max_length=1024, truncation=True, return_tensors=\"tf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HxZ7jOSQCQy"
      },
      "source": [
        "Run the summarizer with the defaults and let's see what it looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wx0og8kI3tOE",
        "outputId": "b9e7e986-29ec-45c7-f508-df46a81064bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Nearly 800 thousand customers are scheduled to be affected by the shutoffs '\n",
            " 'which are expected to last through at least midday tomorrow .<n>PG&E stated '\n",
            " 'it scheduled the blackouts in response to forecasts for high winds amid dry '\n",
            " 'conditions .<n>The aim is to reduce the risk of wildfires .')\n"
          ]
        }
      ],
      "source": [
        "# Generate Summary\n",
        "summary_ids = cnnmodel.generate(inputs[\"input_ids\"]\n",
        ")\n",
        "\n",
        "pprint(cnntokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0], compact=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY8StZx9R1Kx"
      },
      "source": [
        "Let's again experiment with the same set of hyperparameters (but possibly with different values) for the Pegasus system.  It is designed for abstractive summarization and this checkpoint is based on multi-line outputs.  We'll evaluate it against the long reference record.\n",
        "\n",
        "*Your readable multi-line output must have a ROUGE-1 score above 0.25 and a ROUGE-L score above 0.15.*\n",
        "\n",
        "You can use the two cells below to experiment with hyperparameters and generating and scoring your outputs in order to answer questions 3.1 - 3.5 in your answers file.\n",
        "\n",
        "**QUESTION:**\n",
        "\n",
        "3.1 What num_beams value gives you the most readable output that meets the score criteria?\n",
        "\n",
        "3.2 Which no_repeat_ngram_size gives the most readable output that meets the score criteria?\n",
        "\n",
        "3.3 What min_length value gives you the most readable output that meets the score criteria?\n",
        "\n",
        "3.4 Which max_new_tokens value gives you the most readable output that meets the score criteria?\n",
        "\n",
        "3.5 What is the ROUGE-L score associated with your most readable candidate?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZ3IeglUcpu8",
        "outputId": "707edf23-72b1-4470-f446-1cb187c99f38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Nearly 800 thousand customers are scheduled to be affected by the shutoffs '\n",
            " 'which are expected to last through at least midday tomorrow .<n>PG&E stated '\n",
            " 'it scheduled the blackouts in response to forecasts for high winds amid dry '\n",
            " 'conditions .<n>The aim is to reduce the risk of wildfires .')\n"
          ]
        }
      ],
      "source": [
        "# Generate Summary\n",
        "summary_ids = cnnmodel.generate(cnninputs[\"input_ids\"],\n",
        "### YOUR CODE HERE\n",
        "### END YOUR CODE\n",
        "                             )\n",
        "candidate = cnntokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
        "pprint(candidate[0], compact=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtrDFoIKbvG1",
        "outputId": "31900368-98fb-4177-c942-a586be453f0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rouge1': np.float64(0.4000000000000001), 'rouge2': np.float64(0.19417475728155342), 'rougeL': np.float64(0.34285714285714286), 'rougeLsum': np.float64(0.34285714285714286)}\n"
          ]
        }
      ],
      "source": [
        "rouge = evaluate.load('rouge')\n",
        "predictions = candidate\n",
        "references = [LONG_REFERENCE]\n",
        "results = rouge.compute(predictions=predictions,\n",
        "                        references=references)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_8izqsFbQU0"
      },
      "source": [
        "Okay, you're done.\n",
        "\n",
        "Which model do you think produced the best summaries keeping in mind that best is in the eye of the reader?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}